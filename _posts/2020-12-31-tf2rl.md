---
title: 'tf2rl'
date: 2020-12-31
permalink: /posts/python/tf2rl/
excerpt: "tf2rl: TensorFlow2 Reinforcement Learning"
tags:
  - rl
---

# tf2rl: TensorFlow2 Reinforcement Learning

この記事は[強化学習苦手の会 Advent Calendar 2020](https://adventar.org/calendars/5128)の23日目の記事です。

本記事では、tf2rlの開発経緯や開発して良かった点・苦労している点を紹介します。

## tf2rlとは

tf2rlはTensorFlow2系で書かれた深層強化学習ライブラリで、強化学習の研究でよく使われる下記の基本的なアルゴリズムをサポートしています。

- On-policy RL
  - [VPG](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf), [PPO](<https://arxiv.org/abs/1707.06347>)
- Off-policy RL
  - [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (including [DDQN](https://arxiv.org/abs/1509.06461), [Prior. DQN](https://arxiv.org/abs/1511.05952), [Duel. DQN](https://arxiv.org/abs/1511.06581), [Distrib. DQN](<https://arxiv.org/abs/1707.06887>), [Noisy DQN](<https://arxiv.org/abs/1706.10295>))
  - [DDPG](https://arxiv.org/abs/1509.02971) (including [TD3](<https://arxiv.org/abs/1802.09477>), [BiResDDPG](<https://arxiv.org/abs/1905.01072>)), [SAC](<https://arxiv.org/abs/1801.01290>)
  - [CURL](https://arxiv.org/abs/2004.04136)
- Inverse RL
  - [GAIL](<https://arxiv.org/abs/1606.03476>), [GAIfO](<https://arxiv.org/abs/1807.06158>), [VAIL](<https://arxiv.org/abs/1810.00821>)
- Model-based RL
  - [MPC](https://arxiv.org/abs/1708.02596), [ME-TRPO](https://arxiv.org/abs/1802.10592), [iLQG](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf)
- その他
  - [VAE](https://arxiv.org/abs/1312.6114), [D2RL](https://arxiv.org/abs/2010.09163), [Spectral Normalization](<https://arxiv.org/abs/1802.05957>), [ApeX](<https://arxiv.org/abs/1803.00933>), [GAE](https://arxiv.org/abs/1506.02438)

使い方など、もし興味ありましたら[README](https://github.com/keiohta/tf2rl/blob/master/README.md)を参照してください。

## 開発経緯

開発した経緯やモチベーションをまとめました。

- 開発当時（2019年中頃）に色々なRLライブラリを使ったが、あまり自分が気に入るものがなかった
- 理論をより深く理解するために自分でコーディングしたかった（これはめっちゃ良かった）
- TensorFlow2系を使ってみたかった（もともと1系を使っていたがかなり使いにくかった。今もう一度始めるならPyTorchを使います笑）

このように、誰かに使ってもらうというよりは自分の研究開発のために書いたコードでしたが、そこそこ使ってもらっている（2020/12/30時点で327 Star）みたいで良かったです。

### 深層強化学習ライブラリを作った感想

- 良かった点
  - 新しい機能・アルゴリズムを試すのがめちゃくちゃ楽
  - 単純に開発が楽しい（特にStarもらえるとモチベーション上がる）
- 苦労している点
  - 計算機資源不足のため実装したコードの性能評価が困難（特にAtariで実験するDQN系）
  - 業務として開発できないのでコーディング時間の捻出が難しい

## 実行例

使い方は[README](https://github.com/keiohta/tf2rl/blob/master/README.md)に書かれた通りなので、ここではいくつか実際に動かした例を挙げます。

|              Ant SAC              |                      CartPole CURL SAC                       |              Reacher iLQG              |
| :-------------------------------: | :----------------------------------------------------------: | :------------------------------------: |
| ![](/images/20201231_ant_sac.gif) | <img src="/images/20201231_cartpole_curl_sac.gif" height="160px"> | ![](/images/20201231_reacher_ilqg.gif) |

## 今後の予定

以下の機能を実装する予定です。

- Batch/Offline RL
  - BCQ, CQLなど
- Image-input model-free RL
- Google Colabを使ったサンプル（導入を易しくする）

また、細々とSlackなどでサポートしているので興味ある方は気軽に[Twitter](https://twitter.com/ohtake_i)相談ください！