---
title: 'tf2rl'
date: 2020-12-31
permalink: /posts/python/tf2rl/
excerpt: "tf2rl: TensorFlow2 Reinforcement Learning"
tags:
  - rl
---

# tf2rl: TensorFlow2 Reinforcement Learning

[tf2rl](https://github.com/keiohta/tf2rl)はTenforFlow 2.xを使った深層強化学習ライブラリです。

本記事では、tf2rlの開発経緯と使い方を紹介します。

## はじめに

tf2rlはTensorFlow2系で書かれた深層強化学習ライブラリで、強化学習の研究でよく使われる下記のアルゴリズムをサポートしています。

- On-policy RL
  - [VPG](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf), [PPO](<https://arxiv.org/abs/1707.06347>)
- Off-policy RL
  - [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (including [DDQN](https://arxiv.org/abs/1509.06461), [Prior. DQN](https://arxiv.org/abs/1511.05952), [Duel. DQN](https://arxiv.org/abs/1511.06581), [Distrib. DQN](<https://arxiv.org/abs/1707.06887>), [Noisy DQN](<https://arxiv.org/abs/1706.10295>))
  - [DDPG](https://arxiv.org/abs/1509.02971) (including [TD3](<https://arxiv.org/abs/1802.09477>), [BiResDDPG](<https://arxiv.org/abs/1905.01072>)), [SAC](<https://arxiv.org/abs/1801.01290>)
  - [CURL](https://arxiv.org/abs/2004.04136)
- Inverse RL
  - [GAIL](<https://arxiv.org/abs/1606.03476>), [GAIfO](<https://arxiv.org/abs/1807.06158>), [VAIL](<https://arxiv.org/abs/1810.00821>)
- Model-based RL
  - [MPC](https://arxiv.org/abs/1708.02596), [ME-TRPO](https://arxiv.org/abs/1802.10592), [iLQG](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf)
- その他
  - [VAE](https://arxiv.org/abs/1312.6114), [D2RL](https://arxiv.org/abs/2010.09163), [Spectral Normalization](<https://arxiv.org/abs/1802.05957>), [ApeX](<https://arxiv.org/abs/1803.00933>), [GAE](https://arxiv.org/abs/1506.02438)

## 開発経緯

開発した経緯・モチベーションをまとめました。

- 開発当時（2019年中頃）あまり自分が気に入るRLライブラリがなかった
- 理論の理解を助長するために自分でコードに書き起こしたかった
  - 今はたくさんRLライブラリありますが、それでもやって良かったと思います
- TensorFlow2系を使ってみたかった（もともと1系で書いていたが書きにくくて仕方なかった。今もう一度始めるならPyTorch使います笑）

このように、誰かに使ってもらうというよりは自分の勉強のために書いたコードでしたが、そこそこ使ってもらっている（2020/12/30時点で327 Star）みたいで良かったです。

### 深層強化学習ライブラリを作った感想

- 良かった点
  - 新しいアルゴリズムを試すのがめちゃくちゃ楽
  - 開発が楽しい（特にStarもらえるとモチベーション上がる）
- 苦労している点
  - 動作確認するための計算機を持ち合わせていないので論文通りの性能を発揮できるか評価できていない（特にAtariで実験するDQN系）

## 実行例

使い方は[README](https://github.com/keiohta/tf2rl/blob/master/README.md)に書かれた通りなので、ここではいくつか実際に動かした例を挙げます。

| Ant SAC                           | CartPole CURL SAC | Reach iLQG |
| --------------------------------- | ----------------- | ---------- |
| ![](/images/20201230_ant_sac.gif) |                   |            |

## 今後の予定

以下の機能を実装する予定です。

- Batch/Offline RL
  - BCQ, CQLなど
- Image-input model-free RL
- Google Colabを使ったサンプル（導入を易しくする）